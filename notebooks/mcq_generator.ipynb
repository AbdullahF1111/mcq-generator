{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/AbdullahF1111/mcq-generator/blob/main/notebooks/mcq_generator.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# ðŸ§  Automatic MCQ Generator with Transformers\n",
        "This notebook demonstrates how to automatically generate multiple-choice questions (MCQs) from raw text using NLP and Hugging Face models.\n",
        "We'll:\n",
        "- Load NLP models\n",
        "- Extract questions and answers\n",
        "- Generate plausible distractors\n",
        "- Evaluate and visualize results\n"
      ],
      "metadata": {
        "id": "xXB9IDulsty4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Step 1: Install Dependencies\n",
        "We install all required libraries.\n"
      ],
      "metadata": {
        "id": "z_RS1fPesyLd"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iGkenk1OKxLp"
      },
      "outputs": [],
      "source": [
        "\n",
        "!pip install -q transformers sentence-transformers spacy pdfplumber python-docx keybert streamlit\n",
        "!python -m spacy download en_core_web_sm -q\n",
        "!pip install -q lmqg\n",
        "!pip install numpy==1.26.4\n",
        "#"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Step 2: Import and Load Models\n",
        "Here we load transformer models for question generation, distractor generation, and QA validation.\n"
      ],
      "metadata": {
        "id": "Dj_EiXbls4Mo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# mcq_pipeline_final_v2.py\n",
        "\"\"\"\n",
        "Improved MCQ pipeline v2\n",
        "Save as mcq_pipeline_final_v2.py and run in Colab / local env.\n",
        "\n",
        "Requirements (example):\n",
        "pip install lmqg transformers sentence-transformers spacy torch pdfplumber python-docx\n",
        "python -m spacy download en_core_web_sm\n",
        "\"\"\"\n",
        "import re\n",
        "import random\n",
        "import json\n",
        "from typing import List, Dict, Any\n",
        "import torch\n",
        "from lmqg import TransformersQG\n",
        "from transformers import pipeline\n",
        "from sentence_transformers import SentenceTransformer, util\n",
        "import spacy\n",
        "\n",
        "# -------------------------\n",
        "# Config / device\n",
        "# -------------------------\n",
        "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "PIPELINE_DEVICE = 0 if DEVICE == \"cuda\" else -1\n"
      ],
      "metadata": {
        "id": "kMIFSyF6ez3a"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# -------------------------\n",
        "# Load models (may take time)\n",
        "# -------------------------\n",
        "print(\"Loading models... (may take a while)\")\n",
        "# Embeddings (Ù„ØªÙ‚ÙŠÙŠÙ… Ø§Ù„ØªØ´Ø§Ø¨Ù‡)\n",
        "# Move device definition before its first use\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "embedder = SentenceTransformer(\"all-MiniLM-L6-v2\", device=device)\n",
        " #QG model (Ø³Ø¤Ø§Ù„ + Ø¬ÙˆØ§Ø¨)\n",
        "qg = TransformersQG(model=\"lmqg/t5-base-squad-qg\")   # QG + AE\n",
        "\n",
        "qa_pipe = pipeline(\"question-answering\",\n",
        "                   model=\"deepset/roberta-base-squad2\",\n",
        "                   tokenizer=\"deepset/roberta-base-squad2\",\n",
        "                   device=PIPELINE_DEVICE)\n",
        "# fallback generator for distractors (deterministic-ish)\n",
        "distractor_gen = pipeline(\"text2text-generation\",\n",
        "                          model=\"google/flan-t5-base\",\n",
        "                          device=PIPELINE_DEVICE)\n",
        "sbert = SentenceTransformer(\"all-MiniLM-L6-v2\", device=DEVICE)\n",
        "nlp = spacy.load(\"en_core_web_sm\")\n"
      ],
      "metadata": {
        "id": "-DeSw6uMez7y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# -------------------------\n",
        "# Utilities / thresholds\n",
        "# -------------------------\n",
        "NUM_DISTRACTORS = 3\n",
        "SIM_MIN = 0.20\n",
        "SIM_MAX = 0.92\n",
        "PAIRWISE_MAX = 0.86\n",
        "QA_CONF_MIN = 0.05  # minimal score from qa_pipe to accept span correction\n",
        "\n",
        "BLACKLIST_WORDS = {\"option\", \"list\", \"adjectives\", \"unknown\", \"true\", \"false\", \"thing\", \"stuff\"}\n"
      ],
      "metadata": {
        "id": "43477blzez_1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Step 3: Define Helper Functions\n",
        "We'll define text-cleaning, POS-based filtering, and semantic similarity functions.\n"
      ],
      "metadata": {
        "id": "SHf19yKHtNg2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def clean_text_generated(txt: str) -> str:\n",
        "    \"\"\"Basic cleanup of LM outputs: strip, remove weird leading tokens like 'TGlobal' or 'Ä ' chars.\"\"\"\n",
        "    if not txt:\n",
        "        return \"\"\n",
        "    # remove weird non-alphanumeric prefixes\n",
        "    txt = txt.strip()\n",
        "    # fix common tokenizer artifacts\n",
        "    txt = txt.replace(\"Ä \", \" \").replace(\" \", \" \").strip()\n",
        "    # remove stray leading tokens like 'TGlobal' if starts with single letter + uppercase word (heuristic)\n",
        "    txt = re.sub(r'^[A-Z]\\s*([A-Z][a-z]+)', r'\\1', txt)  # 'TGlobal' -> 'Global'\n",
        "    # remove multiple spaces\n",
        "    txt = re.sub(r'\\s+', ' ', txt)\n",
        "    return txt.strip()"
      ],
      "metadata": {
        "id": "3q5TUh5wfA8J"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def is_short_noun_phrase(text: str, max_tokens: int = 3) -> bool:\n",
        "    \"\"\"Return True if text is noun phrase of <= max_tokens and not a sentence.\"\"\"\n",
        "    if not text or len(text.split()) == 0:\n",
        "        return False\n",
        "    if len(text.split()) > max_tokens:\n",
        "        return False\n",
        "    doc = nlp(text)\n",
        "    # reject if contains a verb\n",
        "    if any(tok.pos_ == \"VERB\" for tok in doc):\n",
        "        return False\n",
        "    # require at least one NOUN or PROPN\n",
        "    if not any(tok.pos_ in (\"NOUN\", \"PROPN\") for tok in doc):\n",
        "        return False\n",
        "    # reject if punctuation heavy\n",
        "    if re.fullmatch(r'[^A-Za-z0-9 ]+', text):\n",
        "        return False\n",
        "    return True"
      ],
      "metadata": {
        "id": "393pJ5fPfBA3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def detect_qtype(question: str) -> str:\n",
        "    q = question.lower()\n",
        "    if re.search(r'\\bwhere\\b', q):\n",
        "        return \"LOC\"\n",
        "    if re.search(r'\\bwho\\b|\\bwhom\\b', q):\n",
        "        return \"PERSON\"\n",
        "    if re.search(r'\\bwhen\\b|\\byear\\b|\\b(month|day|morning|evening|summer|winter)\\b', q):\n",
        "        # special-case: 'what ... mountains' should be LOC\n",
        "        if re.search(r'what\\s+.*\\b(mountain|mountains|river|city|lake|island|park|trail|valley|coast|beach|state|country|village|town)\\b', q):\n",
        "            return \"LOC\"\n",
        "        return \"TIME\"\n",
        "    # fallback\n",
        "    return \"OTHER\"\n"
      ],
      "metadata": {
        "id": "EVQC67e9fH03"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Step 4: Define the MCQ generation pipeline\n",
        "This function combines question generation, answer validation, and distractor generation.\n"
      ],
      "metadata": {
        "id": "8qQzWuRCt7TU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def extract_candidates_from_context(context: str) -> List[str]:\n",
        "    doc = nlp(context)\n",
        "    pool = set()\n",
        "    for ent in doc.ents:\n",
        "        txt = ent.text.strip()\n",
        "        if 1 <= len(txt.split()) <= 4:\n",
        "            pool.add(clean_text_generated(txt))\n",
        "    for nc in doc.noun_chunks:\n",
        "        txt = nc.text.strip()\n",
        "        if 1 <= len(txt.split()) <= 4:\n",
        "            pool.add(clean_text_generated(txt))\n",
        "    # add years/numbers found\n",
        "    years = re.findall(r'\\b\\d{4}\\b', context)\n",
        "    for y in years:\n",
        "        pool.add(y)\n",
        "    return list(pool)\n"
      ],
      "metadata": {
        "id": "tTe0BPSFfI6t"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def generate_distractors_by_lm(question: str, answer: str, num: int = 6) -> List[str]:\n",
        "    \"\"\"Prompt LM to produce comma-separated short distractors. Deterministic-ish.\"\"\"\n",
        "    prompt = (f\"Generate {num} short plausible distractors (1-3 words) for this question. \"\n",
        "              f\"Question: {question} | Correct answer: {answer}. \"\n",
        "              f\"Return a comma-separated list. Do NOT repeat the correct answer.\")\n",
        "    try:\n",
        "        out = distractor_gen(prompt, max_new_tokens=64, num_beams=4, do_sample=False, top_p=0.9, temperature=0.7)\n",
        "        txt = out[0].get(\"generated_text\", \"\")\n",
        "        txt = clean_text_generated(txt)\n",
        "        parts = [p.strip() for p in re.split(r'[,\\n;]+', txt) if p.strip()]\n",
        "        # filter very long parts\n",
        "        parts = [p for p in parts if len(p.split()) <= 4]\n",
        "        return parts[:num]\n",
        "    except Exception as e:\n",
        "        print(\"distractor LM error:\", e)\n",
        "        return []"
      ],
      "metadata": {
        "id": "fpuSzdR6fP7R"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def semantic_select(answer: str, candidates: List[str], k: int = NUM_DISTRACTORS) -> List[str]:\n",
        "    \"\"\"Select k candidates semantically close to answer but diverse among themselves.\"\"\"\n",
        "    if not candidates:\n",
        "        return []\n",
        "    cand_emb = sbert.encode(candidates, convert_to_tensor=True)\n",
        "    ans_emb = sbert.encode([answer], convert_to_tensor=True)\n",
        "    sims = util.pytorch_cos_sim(cand_emb, ans_emb).squeeze(1).cpu().numpy()\n",
        "    # pick indices within SIM_MIN..SIM_MAX\n",
        "    idxs = [i for i, s in enumerate(sims) if SIM_MIN <= s <= SIM_MAX]\n",
        "    if not idxs:\n",
        "        idxs = sorted(range(len(candidates)), key=lambda i: -sims[i])[:min(len(candidates), k * 4)]\n",
        "    else:\n",
        "        idxs = sorted(idxs, key=lambda i: -sims[i])\n",
        "    selected = []\n",
        "    for i in idxs:\n",
        "        emb_i = cand_emb[i]\n",
        "        ok = True\n",
        "        for j in selected:\n",
        "            if util.pytorch_cos_sim(emb_i, cand_emb[j]).item() > PAIRWISE_MAX:\n",
        "                ok = False\n",
        "                break\n",
        "        if ok:\n",
        "            selected.append(i)\n",
        "        if len(selected) >= k:\n",
        "            break\n",
        "    return [candidates[i] for i in selected]\n"
      ],
      "metadata": {
        "id": "zLjmnFUAfQAJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def qa_answer_check_and_cleanup(question: str, context: str, answer: str) -> str:\n",
        "    \"\"\"\n",
        "    Use qa_pipe to extract trusted span for the question from context.\n",
        "    If QA returns a confident non-empty answer, prefer it (cleaned).\n",
        "    Otherwise, if provided 'answer' appears verbatim in context, keep it.\n",
        "    Else return empty string (meaning reject this pair).\n",
        "    \"\"\"\n",
        "    try:\n",
        "        res = qa_pipe(question=question, context=context)\n",
        "        pred = res.get(\"answer\", \"\").strip()\n",
        "        score = float(res.get(\"score\", 0.0))\n",
        "        pred = clean_text_generated(pred)\n",
        "        if pred and score >= QA_CONF_MIN:\n",
        "            return pred\n",
        "    except Exception:\n",
        "        pass\n",
        "    # fallback: check if original answer occurs verbatim in context (case-insensitive)\n",
        "    if answer and re.search(re.escape(answer.strip()), context, flags=re.IGNORECASE):\n",
        "        return answer.strip()\n",
        "    return \"\"\n"
      ],
      "metadata": {
        "id": "oz49iPLEfYOR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# -------------------------\n",
        "# Main pipeline function\n",
        "# -------------------------\n",
        "def generate_mcqs_from_text(context: str, num_questions: int = 5, desired_distractors: int = NUM_DISTRACTORS,\n",
        "                            verbose: bool = False) -> Dict[str, Any]:\n",
        "    out = {\"source_len\": len(context.split()), \"questions\": []}\n",
        "    # 1) QG generation (may produce pairs)\n",
        "    try:\n",
        "        qa_pairs_raw = qg.generate_qa(context, num_questions=num_questions)\n",
        "    except Exception as e:\n",
        "        print(\"QG error:\", e)\n",
        "        qa_pairs_raw = []\n",
        "\n",
        "    # convert to uniform (question, answer)\n",
        "    qa_pairs = []\n",
        "    seen_q = set()\n",
        "    for rec in qa_pairs_raw:\n",
        "        if isinstance(rec, dict):\n",
        "            q = clean_text_generated(rec.get(\"question\", \"\"))\n",
        "            a = clean_text_generated(rec.get(\"answer\", \"\"))\n",
        "        elif isinstance(rec, (list, tuple)) and len(rec) == 2:\n",
        "            q = clean_text_generated(rec[0])\n",
        "            a = clean_text_generated(rec[1])\n",
        "        else:\n",
        "            continue\n",
        "        # basic sanity filters\n",
        "        if not q or len(q.split()) < 3:\n",
        "            continue\n",
        "        if q in seen_q:\n",
        "            continue\n",
        "        seen_q.add(q)\n",
        "        qa_pairs.append((q, a))\n",
        "\n",
        "    if verbose:\n",
        "        print(\"QA pairs (cleaned):\", qa_pairs)\n",
        "\n",
        "    pool = extract_candidates_from_context(context)\n",
        "\n",
        "    # process each pair\n",
        "    for q, a in qa_pairs:\n",
        "        # prefer QA-derived answer span if possible\n",
        "        trusted_answer = qa_answer_check_and_cleanup(q, context, a)\n",
        "        if not trusted_answer:\n",
        "            if verbose:\n",
        "                print(\"Reject QA pair (answer not supported in context):\", q, a)\n",
        "            continue\n",
        "        # finalize qtype\n",
        "        qtype = detect_qtype(q)\n",
        "\n",
        "        # build candidate pool for distractors: combine context pool + LM suggestions\n",
        "        lm_cands = generate_distractors_by_lm(q, trusted_answer, num=8)\n",
        "        combined = list(dict.fromkeys(lm_cands + pool))  # preserve order, dedupe\n",
        "\n",
        "        # filter candidates: only accept short noun phrases and not equal/substring of answer or question\n",
        "        filtered = []\n",
        "        qnorm = re.sub(r'[^A-Za-z0-9 ]', '', q).lower()\n",
        "        ansnorm = re.sub(r'[^A-Za-z0-9 ]', '', trusted_answer).lower()\n",
        "        for c in combined:\n",
        "            c_clean = clean_text_generated(c)\n",
        "            if not c_clean:\n",
        "                continue\n",
        "            if len(c_clean) > 30:\n",
        "                continue\n",
        "            if any(b in c_clean.lower() for b in BLACKLIST_WORDS):\n",
        "                continue\n",
        "            # remove if same as answer or contains answer\n",
        "            if ansnorm and ansnorm in c_clean.lower():\n",
        "                continue\n",
        "            # remove if token overlap too high with question (avoid repeating question words)\n",
        "            if sum(1 for w in c_clean.lower().split() if w in qnorm.split()) / max(1, len(c_clean.split())) > 0.6:\n",
        "                continue\n",
        "            # must be short noun phrase\n",
        "            if not is_short_noun_phrase(c_clean, max_tokens=3):\n",
        "                continue\n",
        "            filtered.append(c_clean)\n",
        "\n",
        "        # semantic select\n",
        "        distractors = semantic_select(trusted_answer, filtered, k=desired_distractors)\n",
        "\n",
        "        # QA sanity: avoid distractor that QA returns as answer for the question\n",
        "        try:\n",
        "            # if QA returns same as distractor for this question, drop it\n",
        "            safe = []\n",
        "            for d in distractors:\n",
        "                try:\n",
        "                    res = qa_pipe(question=q, context=context)\n",
        "                    pred = clean_text_generated(res.get(\"answer\", \"\")).lower()\n",
        "                except Exception:\n",
        "                    pred = \"\"\n",
        "                if pred and d.lower() in pred:\n",
        "                    # skip distractor that appears in QA predicted span\n",
        "                    continue\n",
        "                safe.append(d)\n",
        "            distractors = safe\n",
        "        except Exception:\n",
        "            pass\n",
        "\n",
        "        # if not enough distractors, add fillers from pool or fallback LM (and re-clean)\n",
        "        if len(distractors) < desired_distractors:\n",
        "            # try pool\n",
        "            pool_candidates = [p for p in pool if is_short_noun_phrase(p)]\n",
        "            for pc in pool_candidates:\n",
        "                if pc.lower() == trusted_answer.lower(): continue\n",
        "                if pc in distractors: continue\n",
        "                distractors.append(pc)\n",
        "                if len(distractors) >= desired_distractors:\n",
        "                    break\n",
        "\n",
        "        if len(distractors) < desired_distractors:\n",
        "            # ask LM again but force do_sample=False for consistency\n",
        "            extra = generate_distractors_by_lm(q, trusted_answer, num=6)\n",
        "            for e in extra:\n",
        "                if e.lower() == trusted_answer.lower(): continue\n",
        "                if not is_short_noun_phrase(e): continue\n",
        "                if e in distractors: continue\n",
        "                distractors.append(e)\n",
        "                if len(distractors) >= desired_distractors:\n",
        "                    break\n",
        "\n",
        "        # final trim\n",
        "        distractors = distractors[:desired_distractors]\n",
        "        # shuffle options but keep answer known\n",
        "        options = distractors + [trusted_answer]\n",
        "        random.shuffle(options)\n",
        "\n",
        "        out[\"questions\"].append({\n",
        "            \"question\": q,\n",
        "            \"answer\": trusted_answer,\n",
        "            \"options\": options,\n",
        "            \"qtype\": qtype,\n",
        "            \"meta\": {\"pool_used\": len(pool), \"lm_suggested\": len(lm_cands)}\n",
        "        })\n",
        "\n",
        "    return out"
      ],
      "metadata": {
        "id": "oewBot1-fYTV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Step 5: Run Examples\n",
        "We'll test our pipeline on short texts (about clocks, books, etc.)\n"
      ],
      "metadata": {
        "id": "ZxPnB4uRuERI"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kIRJbqPnaL-E"
      },
      "outputs": [],
      "source": [
        "# -------------------------\n",
        "# Example usage (main)\n",
        "# -------------------------\n",
        "if __name__ == \"__main__\":\n",
        "    examples = {\n",
        "        \"clocks\": (\n",
        "            \"The invention of the mechanical clock revolutionized timekeeping. Before clocks, \"\n",
        "            \"people relied on sundials and water clocks. In the 14th century, European inventors \"\n",
        "            \"created gears and weights to measure time more accurately. This allowed societies to \"\n",
        "            \"organize work, prayer, and travel more efficiently. Today, atomic clocks are the most \"\n",
        "            \"precise, used in GPS and scientific research.\"\n",
        "        ),\n",
        "        \"books\": (\n",
        "            \"Reading books is one of the most powerful ways to gain knowledge and expand your imagination. \"\n",
        "        ),\n",
        "        \"liam\": (\n",
        "            \"Liam and his friends hiked through the Rocky Mountains last summer. They camped by a clear lake \"\n",
        "            \"and watched the sunrise one morning. Samuel Harrison promised himself to return every year. The trip \"\n",
        "            \"strengthened their friendship and inspired Liam to study nature.\"\n",
        "        ),\n",
        "        \"climate\": (\n",
        "            \"Global warming is the long-term rise in Earth's average temperature. It is mainly caused by human \"\n",
        "            \"activities such as burning fossil fuels which increase greenhouse gas concentrations in the atmosphere. \"\n",
        "            \"Climate scientists study temperature records, ice cores and other indicators to understand the rate of change.\"\n",
        "        )\n",
        "    }\n",
        "\n",
        "    results_all = {}\n",
        "    for name, text in examples.items():\n",
        "        print(\"\\nProcessing example:\", name)\n",
        "        res = generate_mcqs_from_text(text, num_questions=5, desired_distractors=3, verbose=True)\n",
        "        results_all[name] = res\n",
        "        for i, q in enumerate(res[\"questions\"], 1):\n",
        "            print(f\"{i}. {q['question']}\")\n",
        "            for j, opt in enumerate(q['options']):\n",
        "                print(f\"  {chr(65+j)}. {opt}\")\n",
        "            print(\"  --> Correct:\", q[\"answer\"])\n",
        "            print(\"  meta:\", q[\"meta\"])\n",
        "\n",
        "    with open(\"mcq_output_final_v2.json\", \"w\", encoding=\"utf-8\") as f:\n",
        "        json.dump(results_all, f, ensure_ascii=False, indent=2)\n",
        "    print(\"\\nSaved mcq_output_final_v2.json\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "f0x2l-Hq-erP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "LPbuE1g5l3qn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Step 6: Save Generated MCQs\n",
        "We'll store the results in a JSON file for further use or visualization.\n"
      ],
      "metadata": {
        "id": "3FNlhPUguJpy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from lmqg import TransformersQG\n",
        "\n",
        "qg = TransformersQG(model=\"lmqg/t5-base-squad-qg\")\n",
        "qg.model.save_pretrained(\"mcq-generator/models/qg_model\")\n",
        "qg.tokenizer.save_pretrained(\"mcq-generator/models/tokenizer\")\n"
      ],
      "metadata": {
        "id": "e6Wj6sND7RGd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import json\n",
        "with open(\"mcq_output_final_v2.json\", \"w\", encoding=\"utf-8\") as f:\n",
        "    json.dump(results_all, f, ensure_ascii=False, indent=2)\n",
        "print(\"Saved mcq_output_final_v2.json\")"
      ],
      "metadata": {
        "id": "UNHxS2r5FBWq"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": [],
      "authorship_tag": "ABX9TyM8Hkcy+d77t0IkxXc+c14Z",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}