import streamlit as st
import re
import random
import json
from typing import List, Dict, Any
import torch
from lmqg import TransformersQG
from transformers import pipeline
from sentence_transformers import SentenceTransformer, util
import spacy
import tempfile
import os

# ØªÙƒÙˆÙŠÙ† Ø§Ù„ØµÙØ­Ø©
st.set_page_config(
    page_title="MCQ Generator",
    page_icon="â“",
    layout="wide"
)

# ØªØ­Ù…ÙŠÙ„ Ø§Ù„Ù†Ù…Ø§Ø°Ø¬ Ù…Ø¹ Ø§Ù„ØªØ®Ø²ÙŠÙ† Ø§Ù„Ù…Ø¤Ù‚Øª
@st.cache_resource
def load_models():
    """ØªØ­Ù…ÙŠÙ„ Ø¬Ù…ÙŠØ¹ Ø§Ù„Ù†Ù…Ø§Ø°Ø¬ Ø§Ù„Ù…Ø·Ù„ÙˆØ¨Ø©"""
    device = "cuda" if torch.cuda.is_available() else "cpu"
    
    with st.spinner("Ø¬Ø§Ø±ÙŠ ØªØ­Ù…ÙŠÙ„ Ø§Ù„Ù†Ù…Ø§Ø°Ø¬... Ù‚Ø¯ ÙŠØ³ØªØºØ±Ù‚ Ø¨Ø¶Ø¹ Ø¯Ù‚Ø§Ø¦Ù‚"):
        # Ù†Ù…ÙˆØ°Ø¬ ØªÙˆÙ„ÙŠØ¯ Ø§Ù„Ø£Ø³Ø¦Ù„Ø©
        qg = TransformersQG(model="lmqg/t5-base-squad-qg")
        
        # Ù†Ù…ÙˆØ°Ø¬ Ø§Ù„Ø¥Ø¬Ø§Ø¨Ø§Øª
        qa_pipe = pipeline(
            "question-answering",
            model="deepset/roberta-base-squad2",
            tokenizer="deepset/roberta-base-squad2",
            device=0 if device == "cuda" else -1
        )
        
        # Ù†Ù…ÙˆØ°Ø¬ Ø§Ù„ØªØ¶Ù…ÙŠÙ†
        embedder = SentenceTransformer("all-MiniLM-L6-v2", device=device)
        
        # Ù†Ù…ÙˆØ°Ø¬ spacy
        try:
            nlp = spacy.load("en_core_web_sm")
        except OSError:
            st.error("ÙŠØ¬Ø¨ ØªØ­Ù…ÙŠÙ„ Ù†Ù…ÙˆØ°Ø¬ spacy Ø£ÙˆÙ„Ø§Ù‹. ØªØ´ØºÙŠÙ„: python -m spacy download en_core_web_sm")
            return None
            
        # Ù†Ù…ÙˆØ°Ø¬ ØªÙˆÙ„ÙŠØ¯ Ø§Ù„Ù…Ø´ØªØªØ§Øª
        distractor_gen = pipeline(
            "text2text-generation",
            model="google/flan-t5-base",
            device=0 if device == "cuda" else -1
        )
    
    return {
        'qg': qg,
        'qa_pipe': qa_pipe,
        'embedder': embedder,
        'nlp': nlp,
        'distractor_gen': distractor_gen,
        'device': device
    }

# ØªÙ‡ÙŠØ¦Ø© Ø§Ù„Ø«ÙˆØ§Ø¨Øª
NUM_DISTRACTORS = 3
SIM_MIN = 0.20
SIM_MAX = 0.92
PAIRWISE_MAX = 0.86
QA_CONF_MIN = 0.05

BLACKLIST_WORDS = {"option", "list", "adjectives", "unknown", "true", "false", "thing", "stuff"}

# Ø§Ù„Ø¯ÙˆØ§Ù„ Ø§Ù„Ù…Ø³Ø§Ø¹Ø¯Ø©
def clean_text_generated(txt: str) -> str:
    """ØªÙ†Ø¸ÙŠÙ Ø§Ù„Ù†Øµ Ø§Ù„Ù…ÙˆÙ„Ø¯"""
    if not txt:
        return ""
    txt = txt.strip()
    txt = txt.replace("Ä ", " ").replace(" ", " ").strip()
    txt = re.sub(r'^[A-Z]\s*([A-Z][a-z]+)', r'\1', txt)
    txt = re.sub(r'\s+', ' ', txt)
    return txt.strip()

def is_short_noun_phrase(text: str, max_tokens: int = 3) -> bool:
    """Ø§Ù„ØªØ­Ù‚Ù‚ Ù…Ù† Ø£Ù† Ø§Ù„Ù†Øµ Ø¹Ø¨Ø§Ø±Ø© Ø¹Ù† Ø¬Ù…Ù„Ø© Ø§Ø³Ù…ÙŠØ© Ù‚ØµÙŠØ±Ø©"""
    if not text or len(text.split()) == 0:
        return False
    if len(text.split()) > max_tokens:
        return False
    
    doc = st.session_state.models['nlp'](text)
    if any(tok.pos_ == "VERB" for tok in doc):
        return False
    if not any(tok.pos_ in ("NOUN", "PROPN") for tok in doc):
        return False
    if re.fullmatch(r'[^A-Za-z0-9 ]+', text):
        return False
    return True

def detect_qtype(question: str) -> str:
    """ØªØ­Ø¯ÙŠØ¯ Ù†ÙˆØ¹ Ø§Ù„Ø³Ø¤Ø§Ù„"""
    q = question.lower()
    if re.search(r'\bwhere\b', q):
        return "LOC"
    if re.search(r'\bwho\b|\bwhom\b', q):
        return "PERSON"
    if re.search(r'\bwhen\b|\byear\b|\b(month|day|morning|evening|summer|winter)\b', q):
        if re.search(r'what\s+.*\b(mountain|mountains|river|city|lake|island|park|trail|valley|coast|beach|state|country|village|town)\b', q):
            return "LOC"
        return "TIME"
    return "OTHER"

def extract_candidates_from_context(context: str) -> List[str]:
    """Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ø§Ù„Ù…Ø±Ø´Ø­ÙŠÙ† Ù…Ù† Ø§Ù„Ù†Øµ"""
    doc = st.session_state.models['nlp'](context)
    pool = set()
    
    for ent in doc.ents:
        txt = ent.text.strip()
        if 1 <= len(txt.split()) <= 4:
            pool.add(clean_text_generated(txt))
    
    for nc in doc.noun_chunks:
        txt = nc.text.strip()
        if 1 <= len(txt.split()) <= 4:
            pool.add(clean_text_generated(txt))
    
    years = re.findall(r'\b\d{4}\b', context)
    for y in years:
        pool.add(y)
    
    return list(pool)

def generate_distractors_by_lm(question: str, answer: str, num: int = 6) -> List[str]:
    """ØªÙˆÙ„ÙŠØ¯ Ù…Ø´ØªØªØ§Øª Ø¨Ø§Ø³ØªØ®Ø¯Ø§Ù… Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ Ø§Ù„Ù„ØºÙˆÙŠ"""
    prompt = (f"Generate {num} short plausible distractors (1-3 words) for this question. "
              f"Question: {question} | Correct answer: {answer}. "
              f"Return a comma-separated list. Do NOT repeat the correct answer.")
    try:
        out = st.session_state.models['distractor_gen'](
            prompt, max_new_tokens=64, num_beams=4, do_sample=False, top_p=0.9, temperature=0.7
        )
        txt = out[0].get("generated_text", "")
        txt = clean_text_generated(txt)
        parts = [p.strip() for p in re.split(r'[,\n;]+', txt) if p.strip()]
        parts = [p for p in parts if len(p.split()) <= 4]
        return parts[:num]
    except Exception as e:
        st.error(f"Ø®Ø·Ø£ ÙÙŠ ØªÙˆÙ„ÙŠØ¯ Ø§Ù„Ù…Ø´ØªØªØ§Øª: {e}")
        return []

def semantic_select(answer: str, candidates: List[str], k: int = NUM_DISTRACTORS) -> List[str]:
    """Ø§Ø®ØªÙŠØ§Ø± Ø§Ù„Ù…Ø´ØªØªØ§Øª Ø¨Ù†Ø§Ø¡Ù‹ Ø¹Ù„Ù‰ Ø§Ù„ØªØ´Ø§Ø¨Ù‡ Ø§Ù„Ø¯Ù„Ø§Ù„ÙŠ"""
    if not candidates:
        return []
    
    sbert = st.session_state.models['embedder']
    cand_emb = sbert.encode(candidates, convert_to_tensor=True)
    ans_emb = sbert.encode([answer], convert_to_tensor=True)
    sims = util.pytorch_cos_sim(cand_emb, ans_emb).squeeze(1).cpu().numpy()
    
    idxs = [i for i, s in enumerate(sims) if SIM_MIN <= s <= SIM_MAX]
    if not idxs:
        idxs = sorted(range(len(candidates)), key=lambda i: -sims[i])[:min(len(candidates), k * 4)]
    else:
        idxs = sorted(idxs, key=lambda i: -sims[i])
    
    selected = []
    for i in idxs:
        emb_i = cand_emb[i]
        ok = True
        for j in selected:
            if util.pytorch_cos_sim(emb_i, cand_emb[j]).item() > PAIRWISE_MAX:
                ok = False
                break
        if ok:
            selected.append(i)
        if len(selected) >= k:
            break
    
    return [candidates[i] for i in selected]

def qa_answer_check_and_cleanup(question: str, context: str, answer: str) -> str:
    """Ø§Ù„ØªØ­Ù‚Ù‚ Ù…Ù† Ø§Ù„Ø¥Ø¬Ø§Ø¨Ø© ÙˆØªÙ†Ø¸ÙŠÙÙ‡Ø§"""
    try:
        res = st.session_state.models['qa_pipe'](question=question, context=context)
        pred = res.get("answer", "").strip()
        score = float(res.get("score", 0.0))
        pred = clean_text_generated(pred)
        if pred and score >= QA_CONF_MIN:
            return pred
    except Exception:
        pass
    
    if answer and re.search(re.escape(answer.strip()), context, flags=re.IGNORECASE):
        return answer.strip()
    return ""

def generate_mcqs_from_text(context: str, num_questions: int = 5, desired_distractors: int = NUM_DISTRACTORS) -> Dict[str, Any]:
    """Ø§Ù„Ø¯Ø§Ù„Ø© Ø§Ù„Ø±Ø¦ÙŠØ³ÙŠØ© Ù„ØªÙˆÙ„ÙŠØ¯ Ø§Ù„Ø£Ø³Ø¦Ù„Ø©"""
    out = {"source_len": len(context.split()), "questions": []}
    
    try:
        qa_pairs_raw = st.session_state.models['qg'].generate_qa(context, num_questions=num_questions)
    except Exception as e:
        st.error(f"Ø®Ø·Ø£ ÙÙŠ ØªÙˆÙ„ÙŠØ¯ Ø§Ù„Ø£Ø³Ø¦Ù„Ø©: {e}")
        return out
    
    # Ù…Ø¹Ø§Ù„Ø¬Ø© Ø£Ø²ÙˆØ§Ø¬ Ø§Ù„Ø£Ø³Ø¦Ù„Ø© ÙˆØ§Ù„Ø¥Ø¬Ø§Ø¨Ø§Øª
    qa_pairs = []
    seen_q = set()
    
    for rec in qa_pairs_raw:
        if isinstance(rec, dict):
            q = clean_text_generated(rec.get("question", ""))
            a = clean_text_generated(rec.get("answer", ""))
        elif isinstance(rec, (list, tuple)) and len(rec) == 2:
            q = clean_text_generated(rec[0])
            a = clean_text_generated(rec[1])
        else:
            continue
        
        if not q or len(q.split()) < 3 or q in seen_q:
            continue
        
        seen_q.add(q)
        qa_pairs.append((q, a))
    
    pool = extract_candidates_from_context(context)
    
    # Ù…Ø¹Ø§Ù„Ø¬Ø© ÙƒÙ„ Ø³Ø¤Ø§Ù„
    for q, a in qa_pairs:
        trusted_answer = qa_answer_check_and_cleanup(q, context, a)
        if not trusted_answer:
            continue
        
        qtype = detect_qtype(q)
        lm_cands = generate_distractors_by_lm(q, trusted_answer, num=8)
        combined = list(dict.fromkeys(lm_cands + pool))
        
        # ØªØµÙÙŠØ© Ø§Ù„Ù…Ø±Ø´Ø­ÙŠÙ†
        filtered = []
        qnorm = re.sub(r'[^A-Za-z0-9 ]', '', q).lower()
        ansnorm = re.sub(r'[^A-Za-z0-9 ]', '', trusted_answer).lower()
        
        for c in combined:
            c_clean = clean_text_generated(c)
            if not c_clean or len(c_clean) > 30:
                continue
            if any(b in c_clean.lower() for b in BLACKLIST_WORDS):
                continue
            if ansnorm and ansnorm in c_clean.lower():
                continue
            if sum(1 for w in c_clean.lower().split() if w in qnorm.split()) / max(1, len(c_clean.split())) > 0.6:
                continue
            if not is_short_noun_phrase(c_clean, max_tokens=3):
                continue
            filtered.append(c_clean)
        
        distractors = semantic_select(trusted_answer, filtered, k=desired_distractors)
        
        # Ø¥Ø°Ø§ Ù„Ù… ÙŠÙƒÙ† Ù‡Ù†Ø§Ùƒ Ù…Ø´ØªØªØ§Øª ÙƒØ§ÙÙŠØ©
        if len(distractors) < desired_distractors:
            pool_candidates = [p for p in pool if is_short_noun_phrase(p)]
            for pc in pool_candidates:
                if pc.lower() == trusted_answer.lower() or pc in distractors:
                    continue
                distractors.append(pc)
                if len(distractors) >= desired_distractors:
                    break
        
        # Ø§Ù„Ø®Ù„Ø· Ø§Ù„Ù†Ù‡Ø§Ø¦ÙŠ Ù„Ù„Ø®ÙŠØ§Ø±Ø§Øª
        options = distractors + [trusted_answer]
        random.shuffle(options)
        
        out["questions"].append({
            "question": q,
            "answer": trusted_answer,
            "options": options,
            "qtype": qtype,
            "meta": {"pool_used": len(pool), "lm_suggested": len(lm_cands)}
        })
    
    return out

# ÙˆØ§Ø¬Ù‡Ø© Streamlit
def main():
    st.title("ğŸ§  Ù…ÙˆÙ„Ù‘Ø¯ Ø£Ø³Ø¦Ù„Ø© Ø§Ù„Ø§Ø®ØªÙŠØ§Ø± Ù…Ù† Ù…ØªØ¹Ø¯Ø¯")
    st.markdown("Ø£Ø¯Ø®Ù„ Ø§Ù„Ù†Øµ Ù„Ø¥Ù†Ø´Ø§Ø¡ Ø£Ø³Ø¦Ù„Ø© Ø§Ø®ØªÙŠØ§Ø± Ù…Ù† Ù…ØªØ¹Ø¯Ø¯ ØªÙ„Ù‚Ø§Ø¦ÙŠØ§Ù‹")
    
    # ØªØ­Ù…ÙŠÙ„ Ø§Ù„Ù†Ù…Ø§Ø°Ø¬
    if 'models' not in st.session_state:
        models = load_models()
        if models is None:
            st.stop()
        st.session_state.models = models
    
    # Ø´Ø±ÙŠØ· Ø¬Ø§Ù†Ø¨ÙŠ Ù„Ù„Ø¥Ø¹Ø¯Ø§Ø¯Ø§Øª
    with st.sidebar:
        st.header("Ø§Ù„Ø¥Ø¹Ø¯Ø§Ø¯Ø§Øª")
        num_questions = st.slider("Ø¹Ø¯Ø¯ Ø§Ù„Ø£Ø³Ø¦Ù„Ø©", 1, 10, 5)
        num_distractors = st.slider("Ø¹Ø¯Ø¯ Ø§Ù„Ù…Ø´ØªØªØ§Øª Ù„ÙƒÙ„ Ø³Ø¤Ø§Ù„", 2, 5, 3)
        
        st.header("Ù†ØµÙˆØµ Ù…Ø«Ø§Ù„ÙŠÙ‡")
        example_texts = {
            "Ø§Ù„Ø³Ø§Ø¹Ø§Øª Ø§Ù„Ù…ÙŠÙƒØ§Ù†ÙŠÙƒÙŠØ©": "The invention of the mechanical clock revolutionized timekeeping. Before clocks, people relied on sundials and water clocks. In the 14th century, European inventors created gears and weights to measure time more accurately.",
            "Ø§Ù„Ù‚Ø±Ø§Ø¡Ø©": "Reading books is one of the most powerful ways to gain knowledge and expand your imagination.",
            "Ø§Ù„Ø§Ø­ØªØ¨Ø§Ø³ Ø§Ù„Ø­Ø±Ø§Ø±ÙŠ": "Global warming is the long-term rise in Earth's average temperature. It is mainly caused by human activities such as burning fossil fuels which increase greenhouse gas concentrations."
        }
        
        selected_example = st.selectbox("Ø§Ø®ØªØ± Ù…Ø«Ø§Ù„Ø§Ù‹", list(example_texts.keys()))
        if st.button("Ø§Ø³ØªØ®Ø¯Ø§Ù… Ø§Ù„Ù…Ø«Ø§Ù„"):
            st.session_state.input_text = example_texts[selected_example]
    
    # Ø¥Ø¯Ø®Ø§Ù„ Ø§Ù„Ù†Øµ
    input_text = st.text_area(
        "Ø£Ø¯Ø®Ù„ Ø§Ù„Ù†Øµ Ù‡Ù†Ø§:",
        height=200,
        value=st.session_state.get('input_text', '')
    )
    
    # Ø²Ø± Ø§Ù„ØªÙˆÙ„ÙŠØ¯
    if st.button("ğŸ¯ ØªÙˆÙ„ÙŠØ¯ Ø§Ù„Ø£Ø³Ø¦Ù„Ø©", type="primary"):
        if not input_text.strip():
            st.warning("âš ï¸ Ø§Ù„Ø±Ø¬Ø§Ø¡ Ø¥Ø¯Ø®Ø§Ù„ Ù†Øµ Ø£ÙˆÙ„Ø§Ù‹")
            return
        
        with st.spinner("Ø¬Ø§Ø±ÙŠ ØªÙˆÙ„ÙŠØ¯ Ø§Ù„Ø£Ø³Ø¦Ù„Ø©... Ù‚Ø¯ ÙŠØ³ØªØºØ±Ù‚ Ø¨Ø¶Ø¹ Ø«ÙˆØ§Ù†"):
            results = generate_mcqs_from_text(
                input_text, 
                num_questions=num_questions,
                desired_distractors=num_distractors
            )
        
        # Ø¹Ø±Ø¶ Ø§Ù„Ù†ØªØ§Ø¦Ø¬
        st.subheader(f"Ø§Ù„Ø£Ø³Ø¦Ù„Ø© Ø§Ù„Ù…ÙˆÙ„Ø¯Ø© ({len(results['questions'])} Ø³Ø¤Ø§Ù„)")
        
        if not results['questions']:
            st.info("âš ï¸ Ù„Ù… ÙŠØªÙ… ØªÙˆÙ„ÙŠØ¯ Ø£ÙŠ Ø£Ø³Ø¦Ù„Ø©. Ø­Ø§ÙˆÙ„ Ø¨Ø¥Ø¯Ø®Ø§Ù„ Ù†Øµ Ø£Ø·ÙˆÙ„ Ø£Ùˆ Ø£ÙƒØ«Ø± ØªÙØµÙŠÙ„Ø§Ù‹.")
            return
        
        for i, q in enumerate(results['questions'], 1):
            with st.container():
                st.markdown(f"### Ø§Ù„Ø³Ø¤Ø§Ù„ {i}")
                st.markdown(f"**{q['question']}**")
                
                # Ø¹Ø±Ø¶ Ø§Ù„Ø®ÙŠØ§Ø±Ø§Øª
                cols = st.columns(2)
                for j, option in enumerate(q['options']):
                    with cols[j % 2]:
                        is_correct = option == q['answer']
                        emoji = "âœ…" if is_correct else "âšª"
                        st.markdown(f"{emoji} **{chr(65+j)}.** {option}")
                
                with st.expander("Ù…Ø¹Ù„ÙˆÙ…Ø§Øª Ø¥Ø¶Ø§ÙÙŠØ©"):
                    st.write(f"**Ù†ÙˆØ¹ Ø§Ù„Ø³Ø¤Ø§Ù„:** {q['qtype']}")
                    st.write(f"**Ø§Ù„Ø¥Ø¬Ø§Ø¨Ø© Ø§Ù„ØµØ­ÙŠØ­Ø©:** {q['answer']}")
                    st.write(f"**Ø§Ù„Ù…Ø¹Ù„ÙˆÙ…Ø§Øª:** {q['meta']}")
                
                st.divider()
        
        # Ø®ÙŠØ§Ø±Ø§Øª Ø§Ù„ØªØµØ¯ÙŠØ±
        col1, col2 = st.columns(2)
        
        with col1:
            if st.button("ğŸ“¥ ØªØµØ¯ÙŠØ± ÙƒÙ€ JSON"):
                st.download_button(
                    label="ØªØ­Ù…ÙŠÙ„ JSON",
                    data=json.dumps(results, ensure_ascii=False, indent=2),
                    file_name="mcq_questions.json",
                    mime="application/json"
                )
        
        with col2:
            if st.button("ğŸ“‹ Ù†Ø³Ø® Ø§Ù„Ù†ØªØ§Ø¦Ø¬"):
                output_text = ""
                for i, q in enumerate(results['questions'], 1):
                    output_text += f"{i}. {q['question']}\n"
                    for j, opt in enumerate(q['options']):
                        output_text += f"   {chr(65+j)}. {opt}\n"
                    output_text += f"   Ø§Ù„Ø¥Ø¬Ø§Ø¨Ø©: {q['answer']}\n\n"
                
                st.code(output_text)

if __name__ == "__main__":
    main()
