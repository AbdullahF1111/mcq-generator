import streamlit as st
import re
import random
import json
from typing import List, Dict, Any
import torch
from transformers import pipeline
from sentence_transformers import SentenceTransformer, util

# ØªÙƒÙˆÙŠÙ† Ø§Ù„ØµÙØ­Ø©
st.set_page_config(
    page_title="MCQ Generator",
    page_icon="â“",
    layout="wide"
)

# ØªØ­Ù…ÙŠÙ„ Ø§Ù„Ù†Ù…Ø§Ø°Ø¬ Ù…Ø¹ Ø§Ù„ØªØ®Ø²ÙŠÙ† Ø§Ù„Ù…Ø¤Ù‚Øª
@st.cache_resource
def load_models():
    """ØªØ­Ù…ÙŠÙ„ Ø¬Ù…ÙŠØ¹ Ø§Ù„Ù†Ù…Ø§Ø°Ø¬ Ø§Ù„Ù…Ø·Ù„ÙˆØ¨Ø©"""
    device = "cuda" if torch.cuda.is_available() else "cpu"
    
    with st.spinner("Ø¬Ø§Ø±ÙŠ ØªØ­Ù…ÙŠÙ„ Ø§Ù„Ù†Ù…Ø§Ø°Ø¬... Ù‚Ø¯ ÙŠØ³ØªØºØ±Ù‚ Ø¨Ø¶Ø¹ Ø¯Ù‚Ø§Ø¦Ù‚"):
        try:
            # Ù…Ø­Ø§ÙˆÙ„Ø© ØªØ­Ù…ÙŠÙ„ spacy - Ø¥Ø°Ø§ ÙØ´Ù„ Ù†Ø³ØªØ®Ø¯Ù… Ø¨Ø¯ÙŠÙ„
            try:
                import spacy
                nlp = spacy.load("en_core_web_sm")
                spacy_available = True
            except:
                st.warning("âš ï¸ Ù„Ù… ÙŠØªÙ… ØªØ­Ù…ÙŠÙ„ SpacyØŒ Ø³ÙŠØªÙ… Ø§Ø³ØªØ®Ø¯Ø§Ù… Ù…Ø¹Ø§Ù„Ø¬Ø© Ù†ØµÙŠØ© Ù…Ø¨Ø³Ø·Ø©")
                nlp = None
                spacy_available = False
            
            # Ù†Ù…ÙˆØ°Ø¬ ØªÙˆÙ„ÙŠØ¯ Ø§Ù„Ø£Ø³Ø¦Ù„Ø© Ø¨Ø§Ø³ØªØ®Ø¯Ø§Ù… pipeline Ù…Ø¨Ø§Ø´Ø±Ø©
            qg_pipe = pipeline(
                "text2text-generation",
                model="mrm8488/t5-base-finetuned-question-generation-ap",
                device=0 if device == "cuda" else -1
            )
            
            # Ù†Ù…ÙˆØ°Ø¬ Ø§Ù„Ø¥Ø¬Ø§Ø¨Ø§Øª
            qa_pipe = pipeline(
                "question-answering",
                model="deepset/roberta-base-squad2",
                device=0 if device == "cuda" else -1
            )
            
            # Ù†Ù…ÙˆØ°Ø¬ Ø§Ù„ØªØ¶Ù…ÙŠÙ†
            embedder = SentenceTransformer("all-MiniLM-L6-v2", device=device)
            
            # Ù†Ù…ÙˆØ°Ø¬ ØªÙˆÙ„ÙŠØ¯ Ø§Ù„Ù…Ø´ØªØªØ§Øª
            distractor_gen = pipeline(
                "text2text-generation",
                model="google/flan-t5-base",
                device=0 if device == "cuda" else -1
            )
        
        except Exception as e:
            st.error(f"Ø®Ø·Ø£ ÙÙŠ ØªØ­Ù…ÙŠÙ„ Ø§Ù„Ù†Ù…Ø§Ø°Ø¬: {e}")
            return None
    
    return {
        'qg_pipe': qg_pipe,
        'qa_pipe': qa_pipe,
        'embedder': embedder,
        'nlp': nlp,
        'spacy_available': spacy_available,
        'distractor_gen': distractor_gen,
        'device': device
    }

# Ø§Ù„Ø¯ÙˆØ§Ù„ Ø§Ù„Ù…Ø³Ø§Ø¹Ø¯Ø©
def clean_text_generated(txt: str) -> str:
    """ØªÙ†Ø¸ÙŠÙ Ø§Ù„Ù†Øµ Ø§Ù„Ù…ÙˆÙ„Ø¯"""
    if not txt:
        return ""
    txt = txt.strip()
    txt = re.sub(r'\s+', ' ', txt)
    return txt

def extract_candidates_simple(context: str) -> List[str]:
    """Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ù…Ø±Ø´Ø­ÙŠÙ† Ø¨Ø¯ÙˆÙ† Ø§Ø³ØªØ®Ø¯Ø§Ù… spacy"""
    # Ø§Ø³ØªØ®Ø±Ø§Ø¬ ÙƒÙ„Ù…Ø§Øª ÙƒØ¨ÙŠØ±Ø© (Ø£Ø³Ù…Ø§Ø¡) Ø¨Ø§Ø³ØªØ®Ø¯Ø§Ù… regex Ø¨Ø³ÙŠØ·
    words = re.findall(r'\b[A-Z][a-z]+(?:\s+[A-Z][a-z]+)*\b', context)
    # Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ø£Ø±Ù‚Ø§Ù… Ø³Ù†ÙˆØ§Øª
    years = re.findall(r'\b\d{4}\b', context)
    # Ø§Ø³ØªØ®Ø±Ø§Ø¬ ÙƒÙ„Ù…Ø§Øª Ø´Ø§Ø¦Ø¹Ø© Ø¨Ø¹Ø¯ "the", "a", "an"
    nouns = re.findall(r'\b(?:the|a|an)\s+(\w+\s\w+|\w+)', context.lower())
    nouns = [n.title() for n in nouns]
    
    candidates = list(set(words + years + nouns))
    return [c for c in candidates if 1 <= len(c.split()) <= 3]

def generate_qa_pairs(context: str, num_questions: int = 3) -> List[tuple]:
    """ØªÙˆÙ„ÙŠØ¯ Ø£Ø²ÙˆØ§Ø¬ Ø§Ù„Ø£Ø³Ø¦Ù„Ø© ÙˆØ§Ù„Ø¥Ø¬Ø§Ø¨Ø§Øª"""
    qa_pairs = []
    
    try:
        # ØªÙ‚Ø³ÙŠÙ… Ø§Ù„Ù†Øµ Ø¥Ù„Ù‰ Ø¬Ù…Ù„ Ø¨Ø³ÙŠØ·
        sentences = re.split(r'[.!?]+', context)
        sentences = [s.strip() for s in sentences if len(s.strip().split()) > 5]
        
        for sentence in sentences[:num_questions * 2]:
            # ØªÙˆÙ„ÙŠØ¯ Ø§Ù„Ø³Ø¤Ø§Ù„
            prompt = f"generate question: {sentence}"
            result = st.session_state.models['qg_pipe'](
                prompt,
                max_length=64,
                num_return_sequences=1,
                temperature=0.8
            )
            
            question = clean_text_generated(result[0]['generated_text'])
            
            if question and len(question.split()) >= 3 and '?' in question:
                # Ø§Ù„Ø¹Ø«ÙˆØ± Ø¹Ù„Ù‰ Ø§Ù„Ø¥Ø¬Ø§Ø¨Ø©
                try:
                    qa_result = st.session_state.models['qa_pipe'](
                        question=question,
                        context=context
                    )
                    answer = qa_result.get('answer', '').strip()
                    score = qa_result.get('score', 0)
                    
                    if answer and score > 0.1 and len(answer.split()) <= 4:
                        qa_pairs.append((question, answer))
                except:
                    continue
            
            if len(qa_pairs) >= num_questions:
                break
                
    except Exception as e:
        st.error(f"Ø®Ø·Ø£ ÙÙŠ ØªÙˆÙ„ÙŠØ¯ Ø§Ù„Ø£Ø³Ø¦Ù„Ø©: {e}")
    
    return qa_pairs

def generate_distractors_simple(answer: str, context: str, num: int = 3) -> List[str]:
    """ØªÙˆÙ„ÙŠØ¯ Ù…Ø´ØªØªØ§Øª Ø¨Ø§Ø³ØªØ®Ø¯Ø§Ù… Ø·Ø±ÙŠÙ‚Ø© Ù…Ø¨Ø³Ø·Ø©"""
    candidates = extract_candidates_simple(context)
    
    # Ø¥Ø²Ø§Ù„Ø© Ø§Ù„Ø¥Ø¬Ø§Ø¨Ø© Ø§Ù„ØµØ­ÙŠØ­Ø©
    candidates = [c for c in candidates if c.lower() != answer.lower()]
    
    # Ø§Ø³ØªØ®Ø¯Ø§Ù… Ø§Ù„ØªØ¶Ù…ÙŠÙ† Ø§Ù„Ø¯Ù„Ø§Ù„ÙŠ Ø¥Ø°Ø§ ÙƒØ§Ù† Ù…ØªØ§Ø­Ø§Ù‹
    if len(candidates) > num:
        try:
            sbert = st.session_state.models['embedder']
            cand_emb = sbert.encode(candidates, convert_to_tensor=True)
            ans_emb = sbert.encode([answer], convert_to_tensor=True)
            sims = util.pytorch_cos_sim(cand_emb, ans_emb).squeeze(1).cpu().numpy()
            
            # Ø§Ø®ØªÙŠØ§Ø± Ø§Ù„Ù…Ø±Ø´Ø­ÙŠÙ† Ø§Ù„Ø£ÙƒØ«Ø± ØªØ´Ø§Ø¨Ù‡Ø§Ù‹ (ÙˆÙ„ÙƒÙ† Ù„ÙŠØ³ ÙƒØ«ÙŠØ±Ø§Ù‹)
            selected_indices = []
            for i in sorted(range(len(sims)), key=lambda i: -sims[i]):
                if 0.3 <= sims[i] <= 0.8:
                    selected_indices.append(i)
                if len(selected_indices) >= num:
                    break
            
            if selected_indices:
                return [candidates[i] for i in selected_indices]
        except:
            pass
    
    # Ø¥Ø°Ø§ ÙØ´Ù„ Ø§Ù„Ø§Ø®ØªÙŠØ§Ø± Ø§Ù„Ø¯Ù„Ø§Ù„ÙŠØŒ Ù†Ø¹ÙˆØ¯ Ù„Ù„Ø§Ø®ØªÙŠØ§Ø± Ø§Ù„Ø¹Ø´ÙˆØ§Ø¦ÙŠ
    return candidates[:num]

def generate_mcqs_from_text(context: str, num_questions: int = 3) -> Dict[str, Any]:
    """Ø§Ù„Ø¯Ø§Ù„Ø© Ø§Ù„Ø±Ø¦ÙŠØ³ÙŠØ© Ù„ØªÙˆÙ„ÙŠØ¯ Ø§Ù„Ø£Ø³Ø¦Ù„Ø©"""
    out = {"questions": []}
    
    # ØªÙˆÙ„ÙŠØ¯ Ø£Ø²ÙˆØ§Ø¬ Ø§Ù„Ø£Ø³Ø¦Ù„Ø© ÙˆØ§Ù„Ø¥Ø¬Ø§Ø¨Ø§Øª
    qa_pairs = generate_qa_pairs(context, num_questions)
    
    if not qa_pairs:
        st.info("âš ï¸ Ù„Ù… ÙŠØªÙ… ØªÙˆÙ„ÙŠØ¯ Ø£ÙŠ Ø£Ø³Ø¦Ù„Ø©. Ø­Ø§ÙˆÙ„ Ø¨Ø¥Ø¯Ø®Ø§Ù„ Ù†Øµ Ø£Ø·ÙˆÙ„ Ø£Ùˆ Ø£ÙƒØ«Ø± ØªÙØµÙŠÙ„Ø§Ù‹.")
        return out
    
    for q, a in qa_pairs:
        trusted_answer = clean_text_generated(a)
        if not trusted_answer:
            continue
        
        # ØªÙˆÙ„ÙŠØ¯ Ø§Ù„Ù…Ø´ØªØªØ§Øª
        distractors = generate_distractors_simple(trusted_answer, context, num=3)
        
        # Ø¥Ø°Ø§ Ù„Ù… ÙŠÙƒÙ† Ù‡Ù†Ø§Ùƒ Ù…Ø´ØªØªØ§Øª ÙƒØ§ÙÙŠØ©
        while len(distractors) < 3:
            distractors.append(f"Option {len(distractors)+1}")
        
        # Ø®Ù„Ø· Ø§Ù„Ø®ÙŠØ§Ø±Ø§Øª
        options = distractors + [trusted_answer]
        random.shuffle(options)
        
        out["questions"].append({
            "question": q,
            "answer": trusted_answer,
            "options": options
        })
    
    return out

# ÙˆØ§Ø¬Ù‡Ø© Streamlit
def main():
    st.title("ğŸ§  Ù…ÙˆÙ„Ù‘Ø¯ Ø£Ø³Ø¦Ù„Ø© Ø§Ù„Ø§Ø®ØªÙŠØ§Ø± Ù…Ù† Ù…ØªØ¹Ø¯Ø¯")
    st.markdown("Ø£Ø¯Ø®Ù„ Ø§Ù„Ù†Øµ Ù„Ø¥Ù†Ø´Ø§Ø¡ Ø£Ø³Ø¦Ù„Ø© Ø§Ø®ØªÙŠØ§Ø± Ù…Ù† Ù…ØªØ¹Ø¯Ø¯ ØªÙ„Ù‚Ø§Ø¦ÙŠØ§Ù‹")
    
    # ØªØ­Ù…ÙŠÙ„ Ø§Ù„Ù†Ù…Ø§Ø°Ø¬
    if 'models' not in st.session_state:
        models = load_models()
        if models is None:
            st.error("âŒ ÙØ´Ù„ ÙÙŠ ØªØ­Ù…ÙŠÙ„ Ø§Ù„Ù†Ù…Ø§Ø°Ø¬. Ø§Ù„Ø±Ø¬Ø§Ø¡ Ø§Ù„ØªØ­Ù‚Ù‚ Ù…Ù† Ø§Ù„Ø³Ø¬Ù„Ø§Øª.")
            return
        st.session_state.models = models
        st.success("âœ… ØªÙ… ØªØ­Ù…ÙŠÙ„ Ø§Ù„Ù†Ù…Ø§Ø°Ø¬ Ø¨Ù†Ø¬Ø§Ø­!")
    
    # Ø´Ø±ÙŠØ· Ø¬Ø§Ù†Ø¨ÙŠ Ù„Ù„Ø¥Ø¹Ø¯Ø§Ø¯Ø§Øª
    with st.sidebar:
        st.header("Ø§Ù„Ø¥Ø¹Ø¯Ø§Ø¯Ø§Øª")
        num_questions = st.slider("Ø¹Ø¯Ø¯ Ø§Ù„Ø£Ø³Ø¦Ù„Ø©", 1, 5, 3)
        
        st.header("Ù†ØµÙˆØµ Ù…Ø«Ø§Ù„ÙŠÙ‡")
        example_texts = {
            "Ø§Ù„Ø³Ø§Ø¹Ø§Øª Ø§Ù„Ù…ÙŠÙƒØ§Ù†ÙŠÙƒÙŠØ©": "The invention of the mechanical clock revolutionized timekeeping. Before clocks, people relied on sundials and water clocks. In the 14th century, European inventors created gears and weights to measure time more accurately.",
            "Ø§Ù„Ù‚Ø±Ø§Ø¡Ø©": "Reading books is one of the most powerful ways to gain knowledge and expand your imagination.",
            "Ø§Ù„Ø§Ø­ØªØ¨Ø§Ø³ Ø§Ù„Ø­Ø±Ø§Ø±ÙŠ": "Global warming is the long-term rise in Earth's average temperature. It is mainly caused by human activities such as burning fossil fuels."
        }
        
        selected_example = st.selectbox("Ø§Ø®ØªØ± Ù…Ø«Ø§Ù„Ø§Ù‹", list(example_texts.keys()))
        if st.button("Ø§Ø³ØªØ®Ø¯Ø§Ù… Ø§Ù„Ù…Ø«Ø§Ù„"):
            st.session_state.input_text = example_texts[selected_example]
    
    # Ø¥Ø¯Ø®Ø§Ù„ Ø§Ù„Ù†Øµ
    input_text = st.text_area(
        "Ø£Ø¯Ø®Ù„ Ø§Ù„Ù†Øµ Ù‡Ù†Ø§:",
        height=200,
        value=st.session_state.get('input_text', '')
    )
    
    # Ø²Ø± Ø§Ù„ØªÙˆÙ„ÙŠØ¯
    if st.button("ğŸ¯ ØªÙˆÙ„ÙŠØ¯ Ø§Ù„Ø£Ø³Ø¦Ù„Ø©", type="primary"):
        if not input_text.strip():
            st.warning("âš ï¸ Ø§Ù„Ø±Ø¬Ø§Ø¡ Ø¥Ø¯Ø®Ø§Ù„ Ù†Øµ Ø£ÙˆÙ„Ø§Ù‹")
            return
        
        with st.spinner("Ø¬Ø§Ø±ÙŠ ØªÙˆÙ„ÙŠØ¯ Ø§Ù„Ø£Ø³Ø¦Ù„Ø©... Ù‚Ø¯ ÙŠØ³ØªØºØ±Ù‚ Ø¨Ø¶Ø¹ Ø«ÙˆØ§Ù†"):
            results = generate_mcqs_from_text(input_text, num_questions)
        
        # Ø¹Ø±Ø¶ Ø§Ù„Ù†ØªØ§Ø¦Ø¬
        st.subheader(f"Ø§Ù„Ø£Ø³Ø¦Ù„Ø© Ø§Ù„Ù…ÙˆÙ„Ø¯Ø© ({len(results['questions'])} Ø³Ø¤Ø§Ù„)")
        
        if not results['questions']:
            st.info("âš ï¸ Ù„Ù… ÙŠØªÙ… ØªÙˆÙ„ÙŠØ¯ Ø£ÙŠ Ø£Ø³Ø¦Ù„Ø©. Ø­Ø§ÙˆÙ„ Ø¨Ø¥Ø¯Ø®Ø§Ù„ Ù†Øµ Ø£Ø·ÙˆÙ„ Ø£Ùˆ Ø£ÙƒØ«Ø± ØªÙØµÙŠÙ„Ø§Ù‹.")
            return
        
        for i, q in enumerate(results['questions'], 1):
            with st.container():
                st.markdown(f"### Ø§Ù„Ø³Ø¤Ø§Ù„ {i}")
                st.markdown(f"**{q['question']}**")
                
                # Ø¹Ø±Ø¶ Ø§Ù„Ø®ÙŠØ§Ø±Ø§Øª
                for j, option in enumerate(q['options']):
                    is_correct = option == q['answer']
                    emoji = "âœ…" if is_correct else "âšª"
                    st.markdown(f"{emoji} **{chr(65+j)}.** {option}")
                
                with st.expander("Ù…Ø¹Ù„ÙˆÙ…Ø§Øª Ø§Ù„Ø¥Ø¬Ø§Ø¨Ø©"):
                    st.write(f"**Ø§Ù„Ø¥Ø¬Ø§Ø¨Ø© Ø§Ù„ØµØ­ÙŠØ­Ø©:** {q['answer']}")
                
                st.divider()

if __name__ == "__main__":
    main()
